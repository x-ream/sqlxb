# RAG æœ€ä½³å®è·µæŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£ä»‹ç»å¦‚ä½•ä½¿ç”¨ xb æ„å»ºé«˜æ€§èƒ½çš„ RAGï¼ˆRetrieval-Augmented Generationï¼‰åº”ç”¨ã€‚æ¶µç›–æ–‡æ¡£åˆ†å—ã€å‘é‡å­˜å‚¨ã€æ··åˆæ£€ç´¢ã€é‡æ’åºç­‰å…³é”®æŠ€æœ¯ã€‚

## ğŸ—ï¸ RAG æ¶æ„è®¾è®¡

### å…¸å‹ RAG æµç¨‹

```
ç”¨æˆ·é—®é¢˜ â†’ Embedding â†’ å‘é‡æ£€ç´¢ â†’ é‡æ’åº â†’ ä¸Šä¸‹æ–‡å¢å¼º â†’ LLM ç”Ÿæˆ
```

### xb åœ¨ RAG ä¸­çš„è§’è‰²

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ–‡æ¡£æ‘„å…¥æµç¨‹                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  åŸå§‹æ–‡æ¡£ â†’ åˆ†å— â†’ Embedding â†’ xb.Insert() â†’ Qdrant

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ£€ç´¢æµç¨‹                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  ç”¨æˆ·æŸ¥è¯¢ â†’ Embedding â†’ xb.VectorSearch() 
           â†’ æ ‡é‡è¿‡æ»¤ â†’ é‡æ’åº â†’ è¿”å›ä¸Šä¸‹æ–‡
```

## ğŸ“¦ æ•°æ®æ¨¡å‹è®¾è®¡

### åŸºç¡€ Chunk æ¨¡å‹

```go
package models

import "time"

type DocumentChunk struct {
    ID        int64     `json:"id" db:"id"`
    DocID     *int64    `json:"doc_id" db:"doc_id"`           // åŸæ–‡æ¡£IDï¼ˆéä¸»é”®ï¼Œå¯ä¸º nilï¼‰
    ChunkID   int       `json:"chunk_id" db:"chunk_id"`       // å—åºå·
    Content   string    `json:"content" db:"content"`         // æ–‡æœ¬å†…å®¹
    Embedding []float32 `json:"embedding" db:"embedding"`     // å‘é‡
    
    // å…ƒæ•°æ®å­—æ®µ
    DocType   string    `json:"doc_type" db:"doc_type"`       // æ–‡æ¡£ç±»å‹
    Language  string    `json:"language" db:"language"`       // è¯­è¨€
    Source    string    `json:"source" db:"source"`           // æ¥æº
    Author    string    `json:"author" db:"author"`           // ä½œè€…
    Tags      string    `json:"tags" db:"tags"`               // æ ‡ç­¾ï¼ˆJSONæ•°ç»„ï¼‰
    
    // å±‚çº§ä¿¡æ¯
    Section   string    `json:"section" db:"section"`         // ç« èŠ‚
    Level     int       `json:"level" db:"level"`             // å±‚çº§
    
    // ç»Ÿè®¡ä¿¡æ¯
    TokenCount int      `json:"token_count" db:"token_count"` // Tokenæ•°
    CharCount  int      `json:"char_count" db:"char_count"`   // å­—ç¬¦æ•°
    
    // æ—¶é—´æˆ³
    CreatedAt time.Time `json:"created_at" db:"created_at"`
    UpdatedAt time.Time `json:"updated_at" db:"updated_at"`
}
```

### æ‰©å±•å…ƒæ•°æ®æ¨¡å‹

```go
// ç”¨äºéœ€è¦å¤æ‚å…ƒæ•°æ®çš„åœºæ™¯
type DocumentChunkV2 struct {
    ID         int64              `json:"id"`
    Content    string             `json:"content"`
    Embedding  []float32          `json:"embedding"`
    Metadata   map[string]interface{} `json:"metadata"` // çµæ´»çš„å…ƒæ•°æ®
    CreatedAt  time.Time          `json:"created_at"`
}

// å…ƒæ•°æ®ç¤ºä¾‹
type ChunkMetadata struct {
    DocID       *int64   `json:"doc_id"`  // åŸæ–‡æ¡£IDï¼ˆéä¸»é”®ï¼Œå¯ä¸º nilï¼‰
    DocType     string   `json:"doc_type"`
    Title       string   `json:"title"`
    URL         string   `json:"url"`
    Author      string   `json:"author"`
    PublishDate string   `json:"publish_date"`
    Tags        []string `json:"tags"`
    Section     string   `json:"section"`
    PageNumber  int      `json:"page_number"`
    Language    string   `json:"language"`
}
```

## âœ‚ï¸ æ–‡æ¡£åˆ†å—ç­–ç•¥

### å›ºå®šé•¿åº¦åˆ†å—

```go
func ChunkByFixedSize(text string, chunkSize int, overlap int) []string {
    var chunks []string
    runes := []rune(text)
    
    for i := 0; i < len(runes); i += chunkSize - overlap {
        end := i + chunkSize
        if end > len(runes) {
            end = len(runes)
        }
        chunks = append(chunks, string(runes[i:end]))
        if end == len(runes) {
            break
        }
    }
    
    return chunks
}

// ä½¿ç”¨ç¤ºä¾‹
chunks := ChunkByFixedSize(document, 500, 50) // 500å­—ç¬¦ï¼Œ50å­—ç¬¦é‡å 
```

### è¯­ä¹‰åˆ†å—

```go
func ChunkBySentence(text string, maxSentences int) []string {
    // æŒ‰å¥å­åˆ†å‰²
    sentences := strings.Split(text, "ã€‚")
    
    var chunks []string
    var currentChunk []string
    
    for _, sentence := range sentences {
        currentChunk = append(currentChunk, sentence)
        
        if len(currentChunk) >= maxSentences {
            chunks = append(chunks, strings.Join(currentChunk, "ã€‚")+"ã€‚")
            currentChunk = currentChunk[len(currentChunk)-1:] // ä¿ç•™æœ€åä¸€å¥ä½œä¸ºä¸Šä¸‹æ–‡
        }
    }
    
    if len(currentChunk) > 0 {
        chunks = append(chunks, strings.Join(currentChunk, "ã€‚")+"ã€‚")
    }
    
    return chunks
}
```

### å±‚çº§åˆ†å—ï¼ˆæ¨èï¼‰

```go
type HierarchicalChunk struct {
    Level   int    // 0: æ–‡æ¡£, 1: ç« èŠ‚, 2: æ®µè½, 3: å¥å­
    Content string
    Parent  int64  // çˆ¶çº§ ID
}

func ChunkHierarchical(document string) []HierarchicalChunk {
    var chunks []HierarchicalChunk
    
    // Level 0: å…¨æ–‡æ‘˜è¦
    summary := generateSummary(document)
    chunks = append(chunks, HierarchicalChunk{
        Level:   0,
        Content: summary,
    })
    
    // Level 1: ç« èŠ‚
    sections := splitBySections(document)
    for _, section := range sections {
        chunks = append(chunks, HierarchicalChunk{
            Level:   1,
            Content: section,
            Parent:  0,
        })
        
        // Level 2: æ®µè½
        paragraphs := splitByParagraphs(section)
        for _, para := range paragraphs {
            chunks = append(chunks, HierarchicalChunk{
                Level:   2,
                Content: para,
                Parent:  int64(len(chunks) - 1),
            })
        }
    }
    
    return chunks
}
```

## ğŸ” å‘é‡æ£€ç´¢ç­–ç•¥

### åŸºç¡€å‘é‡æ£€ç´¢

```go
func BasicVectorSearch(query string, embeddingFunc func(string) ([]float32, error)) (map[string]interface{}, error) {
    // ç”ŸæˆæŸ¥è¯¢å‘é‡
    queryVector, err := embeddingFunc(query)
    if err != nil {
        return nil, err
    }
    
    // æ„å»ºæŸ¥è¯¢
    built := xb.Of(&DocumentChunk{}).
        VectorSearch("embedding", queryVector, 10).
        QdrantX(func(qx *xb.QdrantBuilderX) {
            qx.ScoreThreshold(0.7)
        }).
        Build()

    return built.ToQdrantJSON()
}
```

### æ··åˆæ£€ç´¢ï¼ˆå‘é‡ + æ ‡é‡ï¼‰

```go
func HybridSearch(query string, filters SearchFilters, embeddingFunc func(string) ([]float32, error)) (map[string]interface{}, error) {
    queryVector, err := embeddingFunc(query)
    if err != nil {
        return nil, err
    }
    
    builder := xb.Of(&DocumentChunk{}).
        VectorSearch("embedding", queryVector)
    
    // æ ‡é‡è¿‡æ»¤
    // â­ xb è‡ªåŠ¨è¿‡æ»¤ nil/0/ç©ºå­—ç¬¦ä¸²/time.Timeé›¶å€¼/ç©ºåˆ‡ç‰‡ï¼Œç›´æ¥ä¼ å‚
    built := builder.
        Eq("doc_type", filters.DocType).        // è‡ªåŠ¨è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
        Eq("language", filters.Language).       // è‡ªåŠ¨è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
        In("tags", filters.Tags...).            // è‡ªåŠ¨è¿‡æ»¤ç©ºåˆ‡ç‰‡
        Gte("created_at", filters.AfterDate).   // è‡ªåŠ¨è¿‡æ»¤é›¶å€¼
        QdrantX(func(qx *xb.QdrantBuilderX) {
            qx.ScoreThreshold(0.65)
        }).
        Build()

    return built.ToQdrantJSON()
}

type SearchFilters struct {
    DocType   string
    Language  string
    Tags      []string
    AfterDate time.Time
}
```

### å¤šé˜¶æ®µæ£€ç´¢

```go
func MultiStageSearch(query string) ([]DocumentChunk, error) {
    // é˜¶æ®µ1: ç²—å¬å›ï¼ˆå®½æ¾æ¡ä»¶ï¼Œå¤šè¿”å›ç»“æœï¼‰
    built1 := xb.Of(&DocumentChunk{}).
        VectorSearch("embedding", queryVector, 100).
        QdrantX(func(qx *xb.QdrantBuilderX) {
            qx.ScoreThreshold(0.5) // è¾ƒä½é˜ˆå€¼
        }).
        Build()

    stage1JSON, err := built1.ToQdrantJSON()
    if err != nil {
        return nil, err
    }
    
    // æ‰§è¡ŒæŸ¥è¯¢ï¼ˆä¼ªä»£ç ï¼‰
    stage1Results := executeQdrantQuery(stage1JSON)
    
    if err != nil {
        return nil, err
    }
    
    // é˜¶æ®µ2: ç²¾æ’åºï¼ˆä½¿ç”¨æ›´å¤æ‚çš„æ¨¡å‹ï¼‰
    rerankedResults := rerankWithCrossEncoder(query, stage1Results)
    
    // é˜¶æ®µ3: å¤šæ ·æ€§æ§åˆ¶
    diverseResults := applyMMR(rerankedResults, 0.7, 10)
    
    return diverseResults, nil
}
```

### ä¸Šä¸‹æ–‡æ‰©å±•

```go
func SearchWithContext(query string, expandWindow int) ([]DocumentChunk, error) {
    // å…ˆæ‰¾åˆ°æœ€ç›¸å…³çš„ chunks
    relevantChunks, err := BasicVectorSearch(query, embeddingFunc)
    if err != nil {
        return nil, err
    }
    
    var allChunks []DocumentChunk
    
    // ä¸ºæ¯ä¸ªç›¸å…³ chunk è·å–å‰åæ–‡
    for _, chunk := range relevantChunks {
        // è·å–å‰é¢çš„ chunks
        prevChunks, _ := xb.Of(&DocumentChunk{}).
            Eq("doc_id", chunk.DocID).
            Gte("chunk_id", chunk.ChunkID-expandWindow).
            Lt("chunk_id", chunk.ChunkID).
            OrderBy("chunk_id", xb.ASC).
            Build()
        
        // è·å–åé¢çš„ chunks
        nextChunks, _ := xb.Of(&DocumentChunk{}).
            Eq("doc_id", chunk.DocID).
            Gt("chunk_id", chunk.ChunkID).
            Lte("chunk_id", chunk.ChunkID+expandWindow).
            OrderBy("chunk_id", xb.ASC).
            Build()
        
        // åˆå¹¶ä¸Šä¸‹æ–‡
        allChunks = append(allChunks, prevChunks...)
        allChunks = append(allChunks, chunk)
        allChunks = append(allChunks, nextChunks...)
    }
    
    return allChunks, nil
}
```

## ğŸ¯ é‡æ’åºç­–ç•¥

### MMR (Maximal Marginal Relevance)

```go
func ApplyMMR(chunks []DocumentChunk, lambda float64, topK int) []DocumentChunk {
    if len(chunks) == 0 {
        return chunks
    }
    
    selected := []DocumentChunk{chunks[0]} // å…ˆé€‰æ‹©æœ€ç›¸å…³çš„
    remaining := chunks[1:]
    
    for len(selected) < topK && len(remaining) > 0 {
        var bestIdx int
        var bestScore float64 = -1
        
        for i, candidate := range remaining {
            // MMR åˆ†æ•° = Î» * ç›¸å…³æ€§ - (1-Î») * æœ€å¤§ç›¸ä¼¼åº¦
            relevance := candidate.Score
            maxSim := maxSimilarity(candidate, selected)
            
            mmrScore := lambda*relevance - (1-lambda)*maxSim
            
            if mmrScore > bestScore {
                bestScore = mmrScore
                bestIdx = i
            }
        }
        
        selected = append(selected, remaining[bestIdx])
        remaining = append(remaining[:bestIdx], remaining[bestIdx+1:]...)
    }
    
    return selected
}

func maxSimilarity(chunk DocumentChunk, selected []DocumentChunk) float64 {
    var maxSim float64
    for _, s := range selected {
        sim := cosineSimilarity(chunk.Embedding, s.Embedding)
        if sim > maxSim {
            maxSim = sim
        }
    }
    return maxSim
}
```

### Cross-Encoder é‡æ’åº

```go
// ä½¿ç”¨æ›´å¼ºå¤§çš„æ¨¡å‹å¯¹å€™é€‰ç»“æœé‡æ–°è¯„åˆ†
func RerankWithCrossEncoder(query string, chunks []DocumentChunk, model CrossEncoderModel) []DocumentChunk {
    type scoredChunk struct {
        chunk DocumentChunk
        score float64
    }
    
    var scored []scoredChunk
    
    for _, chunk := range chunks {
        // ä½¿ç”¨ Cross-Encoder è®¡ç®—æŸ¥è¯¢å’Œæ–‡æ¡£çš„ç›¸å…³æ€§
        score := model.Score(query, chunk.Content)
        scored = append(scored, scoredChunk{chunk: chunk, score: score})
    }
    
    // æŒ‰åˆ†æ•°æ’åº
    sort.Slice(scored, func(i, j int) bool {
        return scored[i].score > scored[j].score
    })
    
    // è¿”å›é‡æ’åºåçš„ç»“æœ
    var result []DocumentChunk
    for _, s := range scored {
        result = append(result, s.chunk)
    }
    
    return result
}
```

## ğŸ“Š å®Œæ•´ RAG åº”ç”¨ç¤ºä¾‹

### RAG æœåŠ¡

```go
package rag

import (
    "context"
    "github.com/fndome/xb"
)

type RAGService struct {
    db            *sqlx.DB
    qdrantClient  *qdrant.Client
    embeddingFunc func(string) ([]float32, error)
    llmClient     *openai.Client
}

func NewRAGService(db *sqlx.DB, qdrant *qdrant.Client, embedFunc func(string) ([]float32, error), llm *openai.Client) *RAGService {
    return &RAGService{
        db:            db,
        qdrantClient:  qdrant,
        embeddingFunc: embedFunc,
        llmClient:     llm,
    }
}

// å®Œæ•´çš„ RAG æŸ¥è¯¢æµç¨‹
func (s *RAGService) Query(ctx context.Context, query string, options QueryOptions) (*RAGResponse, error) {
    // 1. ç”ŸæˆæŸ¥è¯¢å‘é‡
    queryVector, err := s.embeddingFunc(query)
    if err != nil {
        return nil, fmt.Errorf("embedding error: %w", err)
    }
    
    // 2. å‘é‡æ£€ç´¢ + æ ‡é‡è¿‡æ»¤
    built := xb.Of(&DocumentChunk{}).
        VectorSearch("embedding", queryVector, options.TopK * 2).  // ç²—å¬å›
        Eq("language", options.Language).
        In("doc_type", options.DocTypes...).
        QdrantX(func(qx *xb.QdrantBuilderX) {
            qx.ScoreThreshold(0.6)
        }).
        Build()

    qdrantJSON, err := built.ToQdrantJSON()
    
    if err != nil {
        return nil, err
    }
    
    // 3. æ‰§è¡Œ Qdrant æŸ¥è¯¢
    searchResults, err := s.qdrantClient.Search(ctx, qdrantQuery)
    if err != nil {
        return nil, err
    }
    
    // 4. é‡æ’åºï¼ˆMMRï¼‰
    chunks := parseChunks(searchResults)
    rerankedChunks := ApplyMMR(chunks, 0.7, options.TopK)
    
    // 5. ä¸Šä¸‹æ–‡æ‰©å±•
    expandedContext := s.expandContext(ctx, rerankedChunks, 1)
    
    // 6. æ„å»º Prompt
    prompt := s.buildPrompt(query, expandedContext)
    
    // 7. è°ƒç”¨ LLM ç”Ÿæˆå›ç­”
    answer, err := s.generateAnswer(ctx, prompt)
    if err != nil {
        return nil, err
    }
    
    return &RAGResponse{
        Answer:      answer,
        Sources:     rerankedChunks,
        TokensUsed:  calculateTokens(prompt, answer),
        SearchScore: averageScore(rerankedChunks),
    }, nil
}

func (s *RAGService) buildPrompt(query string, chunks []DocumentChunk) string {
    var contextParts []string
    for i, chunk := range chunks {
        contextParts = append(contextParts, fmt.Sprintf("[æ–‡æ¡£%d]\n%s\n", i+1, chunk.Content))
    }
    
    context := strings.Join(contextParts, "\n")
    
    return fmt.Sprintf(`åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚

æ–‡æ¡£å†…å®¹:
%s

ç”¨æˆ·é—®é¢˜: %s

å›ç­”:`, context, query)
}

func (s *RAGService) generateAnswer(ctx context.Context, prompt string) (string, error) {
    resp, err := s.llmClient.CreateChatCompletion(ctx, openai.ChatCompletionRequest{
        Model: openai.GPT4,
        Messages: []openai.ChatCompletionMessage{
            {
                Role:    openai.ChatMessageRoleSystem,
                Content: "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åŠ©æ‰‹ï¼ŒåŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚",
            },
            {
                Role:    openai.ChatMessageRoleUser,
                Content: prompt,
            },
        },
        Temperature: 0.3,
    })
    
    if err != nil {
        return "", err
    }
    
    return resp.Choices[0].Message.Content, nil
}

type QueryOptions struct {
    Language string
    DocTypes []string
    TopK     int
}

type RAGResponse struct {
    Answer      string
    Sources     []DocumentChunk
    TokensUsed  int
    SearchScore float64
}
```

### HTTP API

```go
package api

import (
    "encoding/json"
    "net/http"
)

type QueryRequest struct {
    Query    string   `json:"query"`
    Language string   `json:"language"`
    DocTypes []string `json:"doc_types"`
    TopK     int      `json:"top_k"`
}

func (h *Handler) HandleRAGQuery(w http.ResponseWriter, r *http.Request) {
    var req QueryRequest
    if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
        http.Error(w, "Invalid request", http.StatusBadRequest)
        return
    }
    
    // é»˜è®¤å€¼
    if req.TopK == 0 {
        req.TopK = 5
    }
    if req.Language == "" {
        req.Language = "zh"
    }
    
    // æ‰§è¡Œ RAG æŸ¥è¯¢
    response, err := h.ragService.Query(r.Context(), req.Query, rag.QueryOptions{
        Language: req.Language,
        DocTypes: req.DocTypes,
        TopK:     req.TopK,
    })
    
    if err != nil {
        http.Error(w, err.Error(), http.StatusInternalServerError)
        return
    }
    
    w.Header().Set("Content-Type", "application/json")
    json.NewEncoder(w).Encode(response)
}
```

## ğŸ¯ æ€§èƒ½ä¼˜åŒ–

### 1. æ‰¹é‡å‘é‡åŒ–

```go
func BatchEmbed(texts []string, batchSize int, embedFunc func([]string) ([][]float32, error)) ([][]float32, error) {
    var allEmbeddings [][]float32
    
    for i := 0; i < len(texts); i += batchSize {
        end := i + batchSize
        if end > len(texts) {
            end = len(texts)
        }
        
        batch := texts[i:end]
        embeddings, err := embedFunc(batch)
        if err != nil {
            return nil, err
        }
        
        allEmbeddings = append(allEmbeddings, embeddings...)
    }
    
    return allEmbeddings, nil
}
```

### 2. ç¼“å­˜ç­–ç•¥

```go
type EmbeddingCache struct {
    cache *lru.Cache
}

func (c *EmbeddingCache) GetOrCompute(text string, computeFunc func(string) ([]float32, error)) ([]float32, error) {
    // ä½¿ç”¨æ–‡æœ¬çš„ hash ä½œä¸º key
    key := hash(text)
    
    if cached, ok := c.cache.Get(key); ok {
        return cached.([]float32), nil
    }
    
    embedding, err := computeFunc(text)
    if err != nil {
        return nil, err
    }
    
    c.cache.Add(key, embedding)
    return embedding, nil
}
```

### 3. å¼‚æ­¥ç´¢å¼•

```go
func (s *RAGService) AsyncIndex(documents []Document) error {
    // ä½¿ç”¨ worker pool å¹¶è¡Œå¤„ç†
    jobs := make(chan Document, len(documents))
    results := make(chan error, len(documents))
    
    // å¯åŠ¨ workers
    for w := 0; w < runtime.NumCPU(); w++ {
        go s.indexWorker(jobs, results)
    }
    
    // å‘é€ä»»åŠ¡
    for _, doc := range documents {
        jobs <- doc
    }
    close(jobs)
    
    // æ”¶é›†ç»“æœ
    for range documents {
        if err := <-results; err != nil {
            return err
        }
    }
    
    return nil
}

func (s *RAGService) indexWorker(jobs <-chan Document, results chan<- error) {
    for doc := range jobs {
        err := s.indexDocument(doc)
        results <- err
    }
}
```

## ğŸ“Š ç›‘æ§ä¸è¯„ä¼°

### æ£€ç´¢è´¨é‡æŒ‡æ ‡

```go
type RetrievalMetrics struct {
    Precision    float64 // å‡†ç¡®ç‡
    Recall       float64 // å¬å›ç‡
    MRR          float64 // Mean Reciprocal Rank
    NDCG         float64 // Normalized Discounted Cumulative Gain
    AvgLatency   time.Duration
}

func EvaluateRetrieval(queries []TestQuery) RetrievalMetrics {
    var metrics RetrievalMetrics
    
    for _, q := range queries {
        results := executeQuery(q.Query)
        
        // è®¡ç®—å„é¡¹æŒ‡æ ‡
        metrics.Precision += calculatePrecision(results, q.RelevantDocs)
        metrics.Recall += calculateRecall(results, q.RelevantDocs)
        metrics.MRR += calculateMRR(results, q.RelevantDocs)
    }
    
    // å¹³å‡å€¼
    n := float64(len(queries))
    metrics.Precision /= n
    metrics.Recall /= n
    metrics.MRR /= n
    
    return metrics
}
```

## ğŸ“ æœ€ä½³å®è·µæ€»ç»“

1. **åˆ†å—ç­–ç•¥**
   - æ¨è 200-500 tokens/chunk
   - ä½¿ç”¨ 50-100 tokens é‡å 
   - è€ƒè™‘å±‚çº§åˆ†å—

2. **å…ƒæ•°æ®è®¾è®¡**
   - æ·»åŠ è¶³å¤Ÿçš„å…ƒæ•°æ®ç”¨äºè¿‡æ»¤
   - ä¿ç•™æ–‡æ¡£å±‚çº§ä¿¡æ¯
   - è®°å½•åˆ†å—ç»Ÿè®¡ä¿¡æ¯

3. **æ£€ç´¢ä¼˜åŒ–**
   - ä½¿ç”¨æ··åˆæ£€ç´¢ï¼ˆå‘é‡+æ ‡é‡ï¼‰
   - å¤šé˜¶æ®µæ£€ç´¢ï¼ˆç²—å¬å›+ç²¾æ’åºï¼‰
   - åº”ç”¨ MMR å¢åŠ å¤šæ ·æ€§

4. **æ€§èƒ½è°ƒä¼˜**
   - æ‰¹é‡å¤„ç†å‘é‡åŒ–
   - ä½¿ç”¨ç¼“å­˜å‡å°‘é‡å¤è®¡ç®—
   - å¼‚æ­¥ç´¢å¼•æé«˜åå

5. **ç›‘æ§è¯„ä¼°**
   - å®šæœŸè¯„ä¼°æ£€ç´¢è´¨é‡
   - ç›‘æ§æŸ¥è¯¢å»¶è¿Ÿ
   - æ”¶é›†ç”¨æˆ·åé¦ˆ

---

**ä¸‹ä¸€æ­¥**: æŸ¥çœ‹ [LANGCHAIN_INTEGRATION.md](./LANGCHAIN_INTEGRATION.md) äº†è§£å¦‚ä½•é›†æˆ LangChainã€‚

