# LlamaIndex é›†æˆæŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£ä»‹ç»å¦‚ä½•å°† sqlxb ä¸ Python LlamaIndex æ¡†æ¶é›†æˆï¼Œæ„å»ºé«˜æ€§èƒ½çš„ RAG å’Œæ•°æ®æŸ¥è¯¢åº”ç”¨ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### è‡ªå®šä¹‰å‘é‡å­˜å‚¨

```python
from llama_index.core.vector_stores import (
    VectorStore,
    VectorStoreQuery,
    VectorStoreQueryResult,
)
from llama_index.core.schema import NodeWithScore, TextNode
from typing import List, Optional, Any
import requests

class SqlxbVectorStore(VectorStore):
    """sqlxb å‘é‡å­˜å‚¨é€‚é…å™¨"""
    
    def __init__(self, backend_url: str, collection_name: str = "default"):
        self.backend_url = backend_url
        self.collection_name = collection_name
    
    def add(self, nodes: List[TextNode], **add_kwargs: Any) -> List[str]:
        """æ·»åŠ èŠ‚ç‚¹"""
        documents = []
        for node in nodes:
            doc = {
                "content": node.get_content(),
                "embedding": node.get_embedding(),
                "metadata": node.metadata,
                "node_id": node.node_id,
            }
            documents.append(doc)
        
        response = requests.post(
            f"{self.backend_url}/api/documents",
            json={"documents": documents, "collection": self.collection_name}
        )
        response.raise_for_status()
        
        return [doc["id"] for doc in response.json()["created"]]
    
    def query(self, query: VectorStoreQuery, **kwargs: Any) -> VectorStoreQueryResult:
        """æ‰§è¡ŒæŸ¥è¯¢"""
        response = requests.post(
            f"{self.backend_url}/api/vector-search",
            json={
                "embedding": query.query_embedding,
                "filters": query.filters or {},
                "top_k": query.similarity_top_k,
                "score_threshold": kwargs.get("score_threshold", 0.0)
            }
        )
        response.raise_for_status()
        
        results = response.json()["results"]
        
        # è½¬æ¢ä¸º LlamaIndex æ ¼å¼
        nodes = []
        similarities = []
        ids = []
        
        for result in results:
            node = TextNode(
                text=result["content"],
                metadata=result["metadata"],
                node_id=result["id"],
            )
            nodes.append(NodeWithScore(node=node, score=result["score"]))
            similarities.append(result["score"])
            ids.append(str(result["id"]))
        
        return VectorStoreQueryResult(
            nodes=nodes,
            similarities=similarities,
            ids=ids
        )
    
    def delete(self, ref_doc_id: str, **delete_kwargs: Any) -> None:
        """åˆ é™¤æ–‡æ¡£"""
        requests.delete(
            f"{self.backend_url}/api/documents/{ref_doc_id}"
        )
```

### åŸºç¡€ RAG åº”ç”¨

```python
from llama_index.core import VectorStoreIndex, ServiceContext, StorageContext
from llama_index.core import SimpleDirectoryReader
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI

# åˆå§‹åŒ–ç»„ä»¶
embed_model = OpenAIEmbedding()
llm = OpenAI(model="gpt-4", temperature=0)

# åˆ›å»ºå‘é‡å­˜å‚¨
vector_store = SqlxbVectorStore(
    backend_url="http://localhost:8080",
    collection_name="my_docs"
)

storage_context = StorageContext.from_defaults(vector_store=vector_store)
service_context = ServiceContext.from_defaults(
    embed_model=embed_model,
    llm=llm
)

# åŠ è½½æ–‡æ¡£
documents = SimpleDirectoryReader("./docs").load_data()

# æ„å»ºç´¢å¼•
index = VectorStoreIndex.from_documents(
    documents,
    storage_context=storage_context,
    service_context=service_context,
)

# æŸ¥è¯¢
query_engine = index.as_query_engine(similarity_top_k=5)
response = query_engine.query("å¦‚ä½•ä½¿ç”¨ sqlxb è¿›è¡Œå‘é‡æ£€ç´¢ï¼Ÿ")

print(response)
```

## ğŸ¯ é«˜çº§åŠŸèƒ½

### æ··åˆæ£€ç´¢

```python
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.query_engine import RetrieverQueryEngine

# åˆ›å»ºæ£€ç´¢å™¨ï¼Œæ”¯æŒå…ƒæ•°æ®è¿‡æ»¤
retriever = VectorIndexRetriever(
    index=index,
    similarity_top_k=10,
    filters={
        "doc_type": "tutorial",
        "language": "zh"
    }
)

# åˆ›å»ºæŸ¥è¯¢å¼•æ“
query_engine = RetrieverQueryEngine.from_args(
    retriever=retriever,
    service_context=service_context
)

response = query_engine.query("sqlxb çš„æ ¸å¿ƒç‰¹æ€§")
```

### å­é—®é¢˜æŸ¥è¯¢

```python
from llama_index.core.query_engine import SubQuestionQueryEngine
from llama_index.core.tools import QueryEngineTool, ToolMetadata

# ä¸ºä¸åŒæ–‡æ¡£ç±»å‹åˆ›å»ºç‹¬ç«‹ç´¢å¼•
tutorial_index = VectorStoreIndex.from_documents(
    tutorial_docs, storage_context=storage_context
)
api_index = VectorStoreIndex.from_documents(
    api_docs, storage_context=storage_context
)

# å®šä¹‰æŸ¥è¯¢å·¥å…·
query_engine_tools = [
    QueryEngineTool(
        query_engine=tutorial_index.as_query_engine(),
        metadata=ToolMetadata(
            name="tutorial_docs",
            description="åŒ…å« sqlxb æ•™ç¨‹å’Œä½¿ç”¨æŒ‡å—"
        ),
    ),
    QueryEngineTool(
        query_engine=api_index.as_query_engine(),
        metadata=ToolMetadata(
            name="api_docs",
            description="åŒ…å« sqlxb API å‚è€ƒæ–‡æ¡£"
        ),
    ),
]

# åˆ›å»ºå­é—®é¢˜æŸ¥è¯¢å¼•æ“
sub_question_engine = SubQuestionQueryEngine.from_defaults(
    query_engine_tools=query_engine_tools,
    service_context=service_context
)

response = sub_question_engine.query(
    "sqlxb å¦‚ä½•é›†æˆ Qdrantï¼Ÿæœ‰å“ªäº› API å¯ä»¥ä½¿ç”¨ï¼Ÿ"
)
```

### èŠå¤©å¼•æ“

```python
from llama_index.core.chat_engine import ContextChatEngine
from llama_index.core.memory import ChatMemoryBuffer

memory = ChatMemoryBuffer.from_defaults(token_limit=3000)

chat_engine = ContextChatEngine.from_defaults(
    retriever=retriever,
    memory=memory,
    service_context=service_context,
)

# å¤šè½®å¯¹è¯
response1 = chat_engine.chat("sqlxb æ”¯æŒå“ªäº›æ•°æ®åº“ï¼Ÿ")
print(response1)

response2 = chat_engine.chat("Qdrant æ€ä¹ˆé›†æˆï¼Ÿ")  # æœ‰ä¸Šä¸‹æ–‡è®°å¿†
print(response2)
```

## ğŸ¤– Agent é›†æˆ

```python
from llama_index.core.agent import ReActAgent
from llama_index.core.tools import FunctionTool

def search_docs(query: str, doc_type: str = "all") -> str:
    """æœç´¢æ–‡æ¡£åº“"""
    filters = {} if doc_type == "all" else {"doc_type": doc_type}
    
    retriever = VectorIndexRetriever(
        index=index,
        similarity_top_k=3,
        filters=filters
    )
    
    nodes = retriever.retrieve(query)
    return "\n\n".join([node.get_content() for node in nodes])

# å®šä¹‰å·¥å…·
tools = [
    FunctionTool.from_defaults(fn=search_docs),
]

# åˆ›å»º Agent
agent = ReActAgent.from_tools(
    tools,
    llm=llm,
    verbose=True
)

# ä½¿ç”¨ Agent
response = agent.chat("å¸®æˆ‘æ‰¾ä¸€ä¸‹å…³äºå‘é‡æ£€ç´¢çš„æ•™ç¨‹")
print(response)
```

## ğŸ“Š å®Œæ•´åº”ç”¨ç¤ºä¾‹

```python
class DocQASystem:
    def __init__(self, backend_url: str):
        self.vector_store = SqlxbVectorStore(backend_url=backend_url)
        self.embed_model = OpenAIEmbedding()
        self.llm = OpenAI(model="gpt-4")
        
        self.storage_context = StorageContext.from_defaults(
            vector_store=self.vector_store
        )
        self.service_context = ServiceContext.from_defaults(
            embed_model=self.embed_model,
            llm=self.llm
        )
        
        self.index = None
    
    def index_directory(self, directory: str):
        """ç´¢å¼•ç›®å½•"""
        documents = SimpleDirectoryReader(directory).load_data()
        
        self.index = VectorStoreIndex.from_documents(
            documents,
            storage_context=self.storage_context,
            service_context=self.service_context,
        )
        
        return len(documents)
    
    def query(self, question: str, filters: dict = None):
        """æŸ¥è¯¢"""
        retriever = VectorIndexRetriever(
            index=self.index,
            similarity_top_k=5,
            filters=filters or {}
        )
        
        query_engine = RetrieverQueryEngine.from_args(
            retriever=retriever,
            service_context=self.service_context
        )
        
        return query_engine.query(question)

# ä½¿ç”¨
qa_system = DocQASystem("http://localhost:8080")
qa_system.index_directory("./docs")
response = qa_system.query("å¦‚ä½•ä½¿ç”¨ sqlxbï¼Ÿ")
print(response)
```

## ğŸ”§ æ€§èƒ½ä¼˜åŒ–

### å¼‚æ­¥æ‰¹é‡å¤„ç†

```python
import asyncio

async def async_index_documents(documents: List):
    """å¼‚æ­¥ç´¢å¼•æ–‡æ¡£"""
    tasks = []
    for doc in documents:
        task = index.ainsert(doc)
        tasks.append(task)
    
    await asyncio.gather(*tasks)

# ä½¿ç”¨
asyncio.run(async_index_documents(documents))
```

### æµå¼å“åº”

```python
# æµå¼æŸ¥è¯¢å“åº”
query_engine = index.as_query_engine(streaming=True)
streaming_response = query_engine.query("sqlxb çš„ç‰¹æ€§")

for text in streaming_response.response_gen:
    print(text, end="", flush=True)
```

## ğŸ“š å‚è€ƒèµ„æº

- [LlamaIndex å®˜æ–¹æ–‡æ¡£](https://docs.llamaindex.ai/)
- [sqlxb ç¤ºä¾‹é¡¹ç›®](https://github.com/x-ream/sqlxb/tree/main/examples)

---

**æç¤º**: ç»“åˆ [LANGCHAIN_INTEGRATION.md](./LANGCHAIN_INTEGRATION.md) æ¯”è¾ƒä¸¤ä¸ªæ¡†æ¶çš„å·®å¼‚ã€‚

