# LangChain é›†æˆæŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£ä»‹ç»å¦‚ä½•å°† sqlxb ä¸ Python LangChain æ¡†æ¶é›†æˆï¼Œæ„å»ºå¼ºå¤§çš„ RAG åº”ç”¨ã€‚

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### é›†æˆæ–¹å¼

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Python LangChain åº”ç”¨              â”‚
â”‚  (Chains, Agents, Memory)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“ HTTP/gRPC
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Go Backend (sqlxb)                â”‚
â”‚  â€¢ VectorSearch API                       â”‚
â”‚  â€¢ Hybrid Search API                      â”‚
â”‚  â€¢ Document Management API                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Qdrant / PostgreSQL+pgvector         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. Go Backend æœåŠ¡

```go
package main

import (
    "encoding/json"
    "net/http"
    "github.com/x-ream/xb"
)

type SearchRequest struct {
    Query         string                 `json:"query"`
    Embedding     []float32              `json:"embedding"`
    Filters       map[string]interface{} `json:"filters"`
    TopK          int                    `json:"top_k"`
    ScoreThreshold float64               `json:"score_threshold"`
}

type SearchResponse struct {
    Results []SearchResult `json:"results"`
}

type SearchResult struct {
    ID       int64                  `json:"id"`
    Content  string                 `json:"content"`
    Metadata map[string]interface{} `json:"metadata"`
    Score    float64                `json:"score"`
}

func handleVectorSearch(w http.ResponseWriter, r *http.Request) {
    var req SearchRequest
    if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
        http.Error(w, err.Error(), http.StatusBadRequest)
        return
    }
    
    // æ„å»ºæŸ¥è¯¢
    builder := sqlxb.Of(&DocumentChunk{}).
        VectorSearch("embedding", req.Embedding)
    
    // æ·»åŠ è¿‡æ»¤æ¡ä»¶
    if docType, ok := req.Filters["doc_type"].(string); ok {
        builder.Eq("doc_type", docType)
    }
    if lang, ok := req.Filters["language"].(string); ok {
        builder.Eq("language", lang)
    }
    
    // ç”Ÿæˆ Qdrant æŸ¥è¯¢
    built := builder.
        QdrantX(func(qx *sqlxb.QdrantBuilderX) {
            qx.ScoreThreshold(float32(req.ScoreThreshold))
        }).
        Build()

    qdrantJSON, err := built.ToQdrantJSON()
    
    if err != nil {
        http.Error(w, err.Error(), http.StatusInternalServerError)
        return
    }
    
    // æ‰§è¡ŒæŸ¥è¯¢ï¼ˆå‡è®¾å·²æœ‰ qdrantClientï¼‰
    results, err := qdrantClient.Search(r.Context(), qdrantQuery)
    if err != nil {
        http.Error(w, err.Error(), http.StatusInternalServerError)
        return
    }
    
    // è½¬æ¢ç»“æœ
    response := SearchResponse{
        Results: convertResults(results),
    }
    
    w.Header().Set("Content-Type", "application/json")
    json.NewEncoder(w).Encode(response)
}

func main() {
    http.HandleFunc("/api/vector-search", handleVectorSearch)
    http.HandleFunc("/api/documents", handleDocumentCRUD)
    http.ListenAndServe(":8080", nil)
}
```

### 2. Python LangChain å®¢æˆ·ç«¯

```python
from langchain.vectorstores.base import VectorStore
from langchain.embeddings.base import Embeddings
from typing import List, Tuple, Optional, Dict, Any
import requests

class SqlxbVectorStore(VectorStore):
    """sqlxb å‘é‡å­˜å‚¨é€‚é…å™¨"""
    
    def __init__(
        self,
        backend_url: str,
        embedding: Embeddings,
        collection_name: str = "default"
    ):
        self.backend_url = backend_url
        self.embedding = embedding
        self.collection_name = collection_name
    
    def add_texts(
        self,
        texts: List[str],
        metadatas: Optional[List[dict]] = None,
        **kwargs: Any
    ) -> List[str]:
        """æ·»åŠ æ–‡æ¡£"""
        embeddings = self.embedding.embed_documents(texts)
        
        documents = []
        for i, (text, emb) in enumerate(zip(texts, embeddings)):
            doc = {
                "content": text,
                "embedding": emb,
                "metadata": metadatas[i] if metadatas else {}
            }
            documents.append(doc)
        
        response = requests.post(
            f"{self.backend_url}/api/documents",
            json={"documents": documents, "collection": self.collection_name}
        )
        response.raise_for_status()
        
        return [str(doc["id"]) for doc in response.json()["created"]]
    
    def similarity_search_with_score(
        self,
        query: str,
        k: int = 4,
        filter: Optional[Dict[str, Any]] = None,
        **kwargs: Any
    ) -> List[Tuple[Document, float]]:
        """ç›¸ä¼¼åº¦æœç´¢ï¼ˆå¸¦åˆ†æ•°ï¼‰"""
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = self.embedding.embed_query(query)
        
        # è°ƒç”¨ sqlxb backend
        response = requests.post(
            f"{self.backend_url}/api/vector-search",
            json={
                "query": query,
                "embedding": query_embedding,
                "filters": filter or {},
                "top_k": k,
                "score_threshold": kwargs.get("score_threshold", 0.0)
            }
        )
        response.raise_for_status()
        
        results = response.json()["results"]
        
        # è½¬æ¢ä¸º LangChain Document æ ¼å¼
        docs_and_scores = []
        for result in results:
            doc = Document(
                page_content=result["content"],
                metadata=result["metadata"]
            )
            docs_and_scores.append((doc, result["score"]))
        
        return docs_and_scores
    
    def similarity_search(
        self,
        query: str,
        k: int = 4,
        **kwargs: Any
    ) -> List[Document]:
        """ç›¸ä¼¼åº¦æœç´¢ï¼ˆä¸å¸¦åˆ†æ•°ï¼‰"""
        docs_and_scores = self.similarity_search_with_score(query, k, **kwargs)
        return [doc for doc, _ in docs_and_scores]
    
    @classmethod
    def from_texts(
        cls,
        texts: List[str],
        embedding: Embeddings,
        metadatas: Optional[List[dict]] = None,
        backend_url: str = "http://localhost:8080",
        **kwargs: Any
    ) -> "SqlxbVectorStore":
        """ä»æ–‡æœ¬åˆ›å»ºå‘é‡å­˜å‚¨"""
        store = cls(backend_url, embedding)
        store.add_texts(texts, metadatas, **kwargs)
        return store
```

### 3. åŸºç¡€ RAG åº”ç”¨

```python
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# 1. åˆå§‹åŒ–ç»„ä»¶
embeddings = OpenAIEmbeddings()
llm = ChatOpenAI(model="gpt-4", temperature=0)

# 2. åˆ›å»ºå‘é‡å­˜å‚¨
vector_store = SqlxbVectorStore(
    backend_url="http://localhost:8080",
    embedding=embeddings,
    collection_name="my_docs"
)

# 3. åŠ è½½å¹¶ç´¢å¼•æ–‡æ¡£
loader = TextLoader("docs/knowledge.txt")
documents = loader.load()

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)
texts = text_splitter.split_documents(documents)

# æ·»åŠ å…ƒæ•°æ®
metadatas = [
    {
        "source": doc.metadata.get("source", ""),
        "doc_type": "tutorial",
        "language": "zh"
    }
    for doc in texts
]

vector_store.add_texts(
    texts=[doc.page_content for doc in texts],
    metadatas=metadatas
)

# 4. åˆ›å»ºæ£€ç´¢é“¾
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vector_store.as_retriever(
        search_kwargs={
            "k": 5,
            "filter": {"language": "zh", "doc_type": "tutorial"}
        }
    ),
    return_source_documents=True
)

# 5. æŸ¥è¯¢
result = qa_chain({"query": "å¦‚ä½•ä½¿ç”¨ sqlxb æ„å»ºå‘é‡æŸ¥è¯¢ï¼Ÿ"})

print(f"å›ç­”: {result['result']}")
print(f"\næ¥æºæ–‡æ¡£:")
for doc in result['source_documents']:
    print(f"  - {doc.metadata['source']}")
```

## ğŸ¯ é«˜çº§ç”¨æ³•

### æ··åˆæ£€ç´¢ï¼ˆHybrid Searchï¼‰

```python
class SqlxbHybridRetriever(BaseRetriever):
    """æ”¯æŒæ ‡é‡è¿‡æ»¤çš„æ··åˆæ£€ç´¢å™¨"""
    
    def __init__(
        self,
        vector_store: SqlxbVectorStore,
        base_filters: Optional[Dict[str, Any]] = None,
        score_threshold: float = 0.7
    ):
        self.vector_store = vector_store
        self.base_filters = base_filters or {}
        self.score_threshold = score_threshold
    
    def get_relevant_documents(self, query: str) -> List[Document]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        # ä»æŸ¥è¯¢ä¸­æå–ç»“æ„åŒ–è¿‡æ»¤æ¡ä»¶
        filters = self._extract_filters(query)
        filters.update(self.base_filters)
        
        # æ‰§è¡Œæ··åˆæ£€ç´¢
        docs_and_scores = self.vector_store.similarity_search_with_score(
            query=query,
            k=20,  # ç²—å¬å›
            filter=filters,
            score_threshold=self.score_threshold
        )
        
        # è¿‡æ»¤ä½åˆ†ç»“æœ
        filtered_docs = [
            doc for doc, score in docs_and_scores
            if score >= self.score_threshold
        ]
        
        return filtered_docs[:5]  # è¿”å› top-5
    
    def _extract_filters(self, query: str) -> Dict[str, Any]:
        """ä»æŸ¥è¯¢ä¸­æå–è¿‡æ»¤æ¡ä»¶ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        filters = {}
        
        # è¯­è¨€æ£€æµ‹
        if contains_chinese(query):
            filters["language"] = "zh"
        else:
            filters["language"] = "en"
        
        # æ–‡æ¡£ç±»å‹è¯†åˆ«
        if "æ•™ç¨‹" in query or "tutorial" in query.lower():
            filters["doc_type"] = "tutorial"
        elif "API" in query.upper():
            filters["doc_type"] = "api"
        
        return filters

# ä½¿ç”¨ç¤ºä¾‹
hybrid_retriever = SqlxbHybridRetriever(
    vector_store=vector_store,
    base_filters={"status": "published"},
    score_threshold=0.75
)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=hybrid_retriever
)

result = qa_chain({"query": "æœ€è¿‘æ›´æ–°çš„ API æ–‡æ¡£"})
```

### å¤šæŸ¥è¯¢æ£€ç´¢ï¼ˆMulti-Queryï¼‰

```python
from langchain.retrievers.multi_query import MultiQueryRetriever

# è‡ªåŠ¨ç”Ÿæˆå¤šä¸ªæŸ¥è¯¢å˜ä½“
multi_query_retriever = MultiQueryRetriever.from_llm(
    retriever=vector_store.as_retriever(),
    llm=llm
)

# å•æ¬¡æŸ¥è¯¢ä¼šè‡ªåŠ¨ç”Ÿæˆå¤šä¸ªå˜ä½“å¹¶åˆå¹¶ç»“æœ
docs = multi_query_retriever.get_relevant_documents(
    "sqlxb å¦‚ä½•å¤„ç†å‘é‡æŸ¥è¯¢ï¼Ÿ"
)
# å†…éƒ¨å¯èƒ½ç”Ÿæˆ:
# - "sqlxb vector search usage"
# - "how to use sqlxb for vector queries"
# - "sqlxb vector query examples"
```

### ä¸Šä¸‹æ–‡å‹ç¼©ï¼ˆContextual Compressionï¼‰

```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

# åˆ›å»ºå‹ç¼©å™¨
compressor = LLMChainExtractor.from_llm(llm)

# åŒ…è£…æ£€ç´¢å™¨
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=vector_store.as_retriever(search_kwargs={"k": 10})
)

# æ£€ç´¢æ—¶è‡ªåŠ¨å‹ç¼©æ–‡æ¡£ï¼Œåªä¿ç•™ç›¸å…³éƒ¨åˆ†
compressed_docs = compression_retriever.get_relevant_documents(
    "sqlxb çš„æ ¸å¿ƒç‰¹æ€§æ˜¯ä»€ä¹ˆï¼Ÿ"
)
```

### è‡ªæŸ¥è¯¢æ£€ç´¢ï¼ˆSelf-Queryingï¼‰

```python
from langchain.chains.query_constructor.base import AttributeInfo
from langchain.retrievers.self_query.base import SelfQueryRetriever

# å®šä¹‰å…ƒæ•°æ®å­—æ®µä¿¡æ¯
metadata_field_info = [
    AttributeInfo(
        name="doc_type",
        description="æ–‡æ¡£ç±»å‹: tutorial, api, blog, faq",
        type="string"
    ),
    AttributeInfo(
        name="language",
        description="æ–‡æ¡£è¯­è¨€: zh, en",
        type="string"
    ),
    AttributeInfo(
        name="created_at",
        description="åˆ›å»ºæ—¶é—´ï¼Œæ ¼å¼ä¸º YYYY-MM-DD",
        type="string"
    ),
    AttributeInfo(
        name="author",
        description="ä½œè€…åç§°",
        type="string"
    ),
]

document_content_description = "sqlxb åº“çš„æŠ€æœ¯æ–‡æ¡£å’Œæ•™ç¨‹"

# åˆ›å»ºè‡ªæŸ¥è¯¢æ£€ç´¢å™¨
self_query_retriever = SelfQueryRetriever.from_llm(
    llm=llm,
    vectorstore=vector_store,
    document_contents=document_content_description,
    metadata_field_info=metadata_field_info,
    verbose=True
)

# è‡ªç„¶è¯­è¨€æŸ¥è¯¢ä¼šè‡ªåŠ¨æå–è¿‡æ»¤æ¡ä»¶
docs = self_query_retriever.get_relevant_documents(
    "æŸ¥æ‰¾2024å¹´å†™çš„å…³äº API çš„ä¸­æ–‡æ•™ç¨‹"
)
# è‡ªåŠ¨æå–è¿‡æ»¤æ¡ä»¶:
# {
#   "doc_type": "tutorial",
#   "language": "zh",
#   "created_at": {"$gte": "2024-01-01"}
# }
```

## ğŸ¤– Agent é›†æˆ

### å°† sqlxb ä½œä¸º Agent å·¥å…·

```python
from langchain.agents import Tool, AgentType, initialize_agent
from langchain.memory import ConversationBufferMemory

# å®šä¹‰å·¥å…·
search_tool = Tool(
    name="KnowledgeBaseSearch",
    func=lambda q: vector_store.similarity_search(q, k=3),
    description="""
    ç”¨äºæœç´¢ sqlxb æŠ€æœ¯æ–‡æ¡£å’Œæ•™ç¨‹ã€‚
    è¾“å…¥åº”è¯¥æ˜¯ä¸€ä¸ªæ¸…æ™°çš„é—®é¢˜æˆ–å…³é”®è¯ã€‚
    è¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µã€‚
    """
)

# åˆ›å»º Agent
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

agent = initialize_agent(
    tools=[search_tool],
    llm=llm,
    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,
    memory=memory,
    verbose=True
)

# å¯¹è¯å¼æŸ¥è¯¢
response = agent.run("sqlxb æ”¯æŒå“ªäº›æ•°æ®åº“ï¼Ÿ")
print(response)

response = agent.run("é‚£ Qdrant çš„é›†æˆæ€ä¹ˆç”¨ï¼Ÿ")  # åŸºäºå†å²ä¸Šä¸‹æ–‡
print(response)
```

### å¤šå·¥å…· Agent

```python
from langchain.tools import StructuredTool

# å®šä¹‰å¤šä¸ªå·¥å…·
search_docs_tool = StructuredTool.from_function(
    func=lambda query, doc_type: vector_store.similarity_search(
        query,
        k=5,
        filter={"doc_type": doc_type}
    ),
    name="SearchDocs",
    description="æœç´¢ç‰¹å®šç±»å‹çš„æ–‡æ¡£ã€‚å‚æ•°: query (str), doc_type (str: tutorial|api|blog|faq)"
)

search_code_tool = StructuredTool.from_function(
    func=lambda query: vector_store.similarity_search(
        query,
        k=3,
        filter={"doc_type": "code_example"}
    ),
    name="SearchCodeExamples",
    description="æœç´¢ä»£ç ç¤ºä¾‹ã€‚å‚æ•°: query (str)"
)

# åˆ›å»ºå¤šå·¥å…· Agent
agent = initialize_agent(
    tools=[search_docs_tool, search_code_tool],
    llm=llm,
    agent=AgentType.OPENAI_FUNCTIONS,
    verbose=True
)

result = agent.run("æˆ‘æƒ³çœ‹çœ‹å¦‚ä½•ä½¿ç”¨ sqlxb è¿›è¡Œå‘é‡æ£€ç´¢çš„ä»£ç ç¤ºä¾‹")
```

## ğŸ“Š å®Œæ•´åº”ç”¨ç¤ºä¾‹

### æ–‡æ¡£é—®ç­”ç³»ç»Ÿ

```python
import os
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

class DocQASystem:
    def __init__(self, backend_url: str, openai_api_key: str):
        self.embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
        self.llm = ChatOpenAI(model="gpt-4", temperature=0, openai_api_key=openai_api_key)
        
        self.vector_store = SqlxbVectorStore(
            backend_url=backend_url,
            embedding=self.embeddings
        )
        
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True,
            output_key="answer"
        )
        
        self.qa_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=self.vector_store.as_retriever(
                search_kwargs={"k": 5, "score_threshold": 0.7}
            ),
            memory=self.memory,
            return_source_documents=True,
            verbose=True
        )
    
    def index_directory(self, directory: str):
        """ç´¢å¼•ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡æ¡£"""
        from langchain.document_loaders import DirectoryLoader, TextLoader
        
        loader = DirectoryLoader(
            directory,
            glob="**/*.md",
            loader_cls=TextLoader
        )
        
        documents = loader.load()
        
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50,
            separators=["\n\n", "\n", " ", ""]
        )
        
        splits = text_splitter.split_documents(documents)
        
        # æ·»åŠ å…ƒæ•°æ®
        for doc in splits:
            doc.metadata.update({
                "language": "zh" if contains_chinese(doc.page_content) else "en",
                "doc_type": self._infer_doc_type(doc),
                "indexed_at": datetime.now().isoformat()
            })
        
        self.vector_store.add_documents(splits)
        
        return len(splits)
    
    def query(self, question: str, filters: Optional[Dict] = None) -> Dict:
        """æŸ¥è¯¢æ–‡æ¡£"""
        if filters:
            # ä¸´æ—¶æ›´æ–°æ£€ç´¢å™¨çš„è¿‡æ»¤æ¡ä»¶
            self.qa_chain.retriever.search_kwargs["filter"] = filters
        
        result = self.qa_chain({"question": question})
        
        return {
            "answer": result["answer"],
            "sources": [
                {
                    "content": doc.page_content[:200] + "...",
                    "metadata": doc.metadata
                }
                for doc in result["source_documents"]
            ]
        }
    
    def _infer_doc_type(self, doc) -> str:
        """æ¨æ–­æ–‡æ¡£ç±»å‹"""
        filename = doc.metadata.get("source", "").lower()
        
        if "tutorial" in filename or "guide" in filename:
            return "tutorial"
        elif "api" in filename:
            return "api"
        elif "blog" in filename:
            return "blog"
        elif "faq" in filename:
            return "faq"
        else:
            return "general"

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    qa_system = DocQASystem(
        backend_url="http://localhost:8080",
        openai_api_key=os.getenv("OPENAI_API_KEY")
    )
    
    # ç´¢å¼•æ–‡æ¡£
    print("æ­£åœ¨ç´¢å¼•æ–‡æ¡£...")
    num_chunks = qa_system.index_directory("./docs")
    print(f"å·²ç´¢å¼• {num_chunks} ä¸ªæ–‡æ¡£å—")
    
    # äº¤äº’å¼é—®ç­”
    print("\næ–‡æ¡£é—®ç­”ç³»ç»Ÿå·²å°±ç»ªã€‚è¾“å…¥ 'quit' é€€å‡ºã€‚\n")
    
    while True:
        question = input("é—®é¢˜: ")
        if question.lower() in ["quit", "exit"]:
            break
        
        result = qa_system.query(
            question,
            filters={"doc_type": "tutorial"}  # åªæœç´¢æ•™ç¨‹
        )
        
        print(f"\nå›ç­”: {result['answer']}\n")
        print("æ¥æº:")
        for i, source in enumerate(result['sources'], 1):
            print(f"  [{i}] {source['metadata']['source']}")
        print()
```

## ğŸ”§ æ€§èƒ½ä¼˜åŒ–

### 1. æ‰¹é‡Embedding

```python
# æ‰¹é‡ç”Ÿæˆ embedding æé«˜æ•ˆç‡
texts = [doc.page_content for doc in documents]

# æ¯æ¬¡å¤„ç† 100 ä¸ª
batch_size = 100
all_embeddings = []

for i in range(0, len(texts), batch_size):
    batch = texts[i:i+batch_size]
    embeddings = embeddings_model.embed_documents(batch)
    all_embeddings.extend(embeddings)

# æ‰¹é‡æ’å…¥
vector_store.add_texts_with_embeddings(texts, all_embeddings, metadatas)
```

### 2. å¼‚æ­¥å¤„ç†

```python
import asyncio
from langchain.embeddings import OpenAIEmbeddings

class AsyncSqlxbVectorStore(SqlxbVectorStore):
    async def aadd_texts(
        self,
        texts: List[str],
        metadatas: Optional[List[dict]] = None,
        **kwargs
    ) -> List[str]:
        """å¼‚æ­¥æ·»åŠ æ–‡æ¡£"""
        embeddings = await self.embedding.aembed_documents(texts)
        
        # ... å¼‚æ­¥ HTTP è¯·æ±‚
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.backend_url}/api/documents",
                json={"documents": documents}
            ) as response:
                result = await response.json()
                return [str(doc["id"]) for doc in result["created"]]

# ä½¿ç”¨å¼‚æ­¥ç‰ˆæœ¬
async def index_documents_async(docs):
    tasks = [
        vector_store.aadd_texts([doc.page_content], [doc.metadata])
        for doc in docs
    ]
    await asyncio.gather(*tasks)

asyncio.run(index_documents_async(documents))
```

## ğŸ“š å®Œæ•´é¡¹ç›®æ¨¡æ¿

æŸ¥çœ‹ `examples/langchain-rag-app/` ç›®å½•è·å–å®Œæ•´çš„é¡¹ç›®æ¨¡æ¿ï¼ŒåŒ…æ‹¬:

- âœ… Go Backend API (ä½¿ç”¨ sqlxb)
- âœ… Python LangChain å®¢æˆ·ç«¯
- âœ… FastAPI REST API
- âœ… Streamlit Web UI
- âœ… Docker Compose éƒ¨ç½²é…ç½®
- âœ… å®Œæ•´æµ‹è¯•å¥—ä»¶

## ğŸ¤ ç¤¾åŒºèµ„æº

- [LangChain å®˜æ–¹æ–‡æ¡£](https://python.langchain.com/)
- [sqlxb ç¤ºä¾‹ä»“åº“](https://github.com/x-ream/xb-examples)
- [å¸¸è§é—®é¢˜è§£ç­”](./FAQ.md)

---

**ä¸‹ä¸€æ­¥**: æŸ¥çœ‹ [LLAMAINDEX_INTEGRATION.md](./LLAMAINDEX_INTEGRATION.md) äº†è§£ LlamaIndex é›†æˆã€‚

